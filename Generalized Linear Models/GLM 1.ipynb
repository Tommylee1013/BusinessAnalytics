{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Continuous Distribution\n",
    "\n",
    "Binomial Distribution의 정준연결함수는 $\\log \\frac{p}{1-p}$, Poisson Distribution의 정준연결함수는 $\\log \\lambda$이다\n",
    "\n",
    "- Continuous의 대표적 분포는 정규분포이고, Discrete의 대표적 분포는 이항분포와 포아송 분포가 있다\n",
    "\n",
    "포아송 분포의 예시 : 동전을 던지는 횟수가 무한히 많다고 가정할 때, 옆면이 나올 확률 (즉, 매우 희소한 사건의 확률을 추정)\n",
    "\n",
    "$$Z \\sim \\mathrm{Bin}(k \\cdot u, \\frac{p}{k})$$\n",
    "$$Z \\sim \\mathrm{Bin}(up, up(1 - \\frac{p}{k}))$$\n",
    "\n",
    "이때, $k$가 무한히 나가면 Poisson 분포를 따른다고 한다. 여기서 $\\lambda = up$이다\n",
    "\n",
    "$$Z \\sim \\mathrm{Poisson}(\\lambda)$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83c4d417c5e61512"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 이항 분포 : 자료의 평균값과 자료의 최대값이 비교가능하다. $Z$가 나올 최대값은 $ku$, 평균은 $pu$이다 \n",
    "- 포아송 분포 : 평균은 $up$, 최대값은 무한대에 가까움으로 평균 대비 최대값의 차이가 엄청 크다\n",
    "\n",
    "$\\rightarrow$ 이런 것을 이용해서 포아송분포와 이항분포를 어느정도 구별할 수 있다. 분포의 가정때 사용하자\n",
    "\n",
    "포아송 분포가 더 기본적이고, 이항분포는 포아송 분포를 도출할때 만들어낼 수 있다\n",
    "\n",
    "$$y_1 \\sim \\mathrm{Poisson}(\\lambda_1)$$\n",
    "$$y_2 \\sim \\mathrm{Poisson}(\\lambda_2)$$\n",
    "\n",
    "$y_1 + y_2 = u$라고 제약식이 존재하게 되면 $y_1|y_1+y_2$은 이항분포를 따른다. 제약식이 있으면 최대값이 생기기 때문이다.\n",
    "\n",
    "$$y_1 | y_1 + y_2 \\sim \\mathrm{Bin}(y_1 + y_2, \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2})$$\n",
    "\n",
    "$\\log \\lambda_1 = \\eta_1$이고 $\\lambda_1 = e^{\\eta_1}$이므로, $\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} = \\frac{e^{\\eta_1}}{e^{\\eta_1} + e^{\\eta_2}}$이다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fed64ce072d1632"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MultiClass Distribution\n",
    "\n",
    "$$(y_1, y_2, y_3)|(y_1 + y_2 + y_3 = u) \\sim \\mathrm{Mnom}(u, (p_1, p_2, p_3))$$\n",
    "$$p_1 = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2 + \\lambda_3} = \\frac{e^{\\eta_1}}{e^{\\eta_1} + e^{\\eta_2} + e^{\\eta_3}} = \\frac{1}{1 + e^{\\delta_1} + e^{\\delta_2}}$$\n",
    "$$p_2 = \\frac{\\lambda_2}{\\lambda_1 + \\lambda_2 + \\lambda_3} = \\frac{e^{\\eta_2}}{e^{\\eta_1} + e^{\\eta_2} + e^{\\eta_3}} = \\frac{e^{\\delta_1}}{1 + e^{\\delta_1} + e^{\\delta_2}}$$\n",
    "$$p_3 = \\frac{\\lambda_3}{\\lambda_1 + \\lambda_2 + \\lambda_3} = \\frac{e^{\\eta_3}}{e^{\\eta_1} + e^{\\eta_2} + e^{\\eta_3}} = \\frac{e^{\\delta_2}}{1 + e^{\\delta_1} + e^{\\delta_2}}$$\n",
    "\n",
    "마지막 항은 약분하는 과정이 포함된 것이다. 이것을 Softmax 함수라고 한다\n",
    "\n",
    "$$\\mathbf{p} = (p_1, p_2, p_3) \\Rightarrow \\mathrm{Softmax} \\Rightarrow\\mathbf{\\delta} = (\\delta_1, \\delta_2)$$\n",
    "\n",
    "위의 과정은 간편하게 아래와 같이 표현할 수 있다\n",
    "\n",
    "$$\\mathbf{p} = \\mathbf{p}(\\mathbf{\\delta}) = [\\mathbf{\\delta}] = [\\mathbf{\\delta}]_*$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50f2ba9592685629"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Binomial / Multinomial Logistic Regression\n",
    "\n",
    "$$\\mathbf{z}_i = (z_1, z_2, z_3) \\sim \\mathrm{Mnom}(1, \\mathbf{p}_i), ~~~~~ \\Sigma_k z_{ik} = 1, ~~ i = 1, \\dots, u$$\n",
    "$$\\mathbf{p}_i = (p_{i1}, p_{i2}, p_{i3}) = \\mathrm{Softmax}(\\mathbf{\\delta}_i)$$\n",
    "$$\\mathbf{\\delta}_i = (\\delta_{i1}, \\delta_{i2}) = (\\mathbf{x}_i\\beta_1, \\mathbf{x}_i\\beta_2) = \\mathbf{x}_i(\\mathbf{\\beta}_1, \\mathbf{\\beta}_2) = \\mathbf{x}_i B$$\n",
    "\n",
    "예시로, 주사위를 던질 때마다 특정한 숫자가 나올 확률이 달라진다\n",
    "\n",
    "분류와 회귀는 사실 다르지 않다\n",
    "\n",
    "연결함수를 뒤집으면 Activation Function이라고 부른다. 분류모형과 회귀모형의 다른점은 바로 activation function의 존재 유무이다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fe41f658b23f732"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 여러 모형들\n",
    "\n",
    "$$\\mathbf{D} = \\{(y_i, \\mathbf{x}_i), i = 1, 2, \\dots, n\\}$$\n",
    "$$\\mathbf{D} = \\{(\\mathbf{y}_i, \\mathbf{x}_i), i = 1, 2, \\dots, n\\}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$y_i \\sim N(\\mu, \\sigma^2), ~~~~~~ y_i \\sim N(\\mathbf{x}_i\\mathbf{\\beta}, \\sigma^2)$$\n",
    "\n",
    "뒤쪽 모형은 시행 시마다 평균이 달라진다는 의미이다. 즉, 회귀분석이다\n",
    "\n",
    "$$\\mathbf{y}_i \\sim N_m(\\mathbf{\\mu}, \\mathbf{\\Sigma}^2), ~~~~~~ y_i \\sim N_m(\\mathbf{x}_i\\mathbf{B}, \\mathbf{\\Sigma}^2)$$\n",
    "\n",
    "뒤쪽의 모형은 다변량 회귀분석이라고 한다\n",
    "\n",
    "$$y_i \\sim \\mathrm{Bin}(u,p), ~~~~~~ y_i \\sim \\mathrm{Bin}(u, [\\mathbf{x}_i, \\mathbf{\\beta}])$$\n",
    "\n",
    "뒤쪽 모형은 logistic regression이라고 한다\n",
    "\n",
    "$$\\mathbf{y}_i \\sim \\mathrm{Bin}(u,\\mathbf{p}), ~~~~~~ y_i \\sim \\mathrm{Bin}(u, [\\mathbf{x}_i, B]_*)$$\n",
    "\n",
    "뒤쪽의 모형이 다항분포 logistic regression이라고 한다. 즉 ,MultiClass Classification이다\n",
    "\n",
    "제약식이 존재하는 경우(클래스 발생확률의 합은 1이다), Softmax를 써 주는 것이 좋다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d688dbfebce864fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "$$y_i \\sim \\mathrm{Poisson}(\\lambda_i), ~~~~~ f(y_i,|\\lambda_i) = e^{-\\lambda_i} \\frac{\\lambda_i^{y_i}}{y_i !}$$\n",
    "\n",
    "$y_i = (y_1, y_2, \\dots, y_n)$일 때, $f(y | \\lambda) = f(y_1 | \\lambda_1) + f(y_2 | \\lambda_2) + \\cdots + f(y_n | \\lambda_n) $\n",
    "\n",
    "식을 간편하게 하기 위해 양번에 log를 취하면\n",
    "\n",
    "$$\\log f(y|\\lambda) = \\sum_{i}^n \\log f(y_i|\\lambda_i)$$\n",
    "\n",
    "관측치 $y_i$는 알고 있지만, $\\lambda_i$는 알지 못하는 상태이다. 여기서 $\\log \\lambda_i = \\alpha + \\beta x_i$라고 가정하면, $\\lambda_i = e^{\\alpha + \\beta x_i}$이다\n",
    "\n",
    "$\\lambda$추정치를 최대화하는 $\\alpha, \\beta$값을 추정하는데, optimization target을 $-\\lambda_i$로 두면 최적화 문제로 풀 수 있다. 이를 최대 우도 추정법이라고 한다\n",
    "\n",
    "데이터가 정규분포인 경우, 최대우도추정량과 최소자승추정량은 평균에 대한 추정치는 같아지고 분산에 대한 추정치는 달라진다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4103e03e427e15ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
